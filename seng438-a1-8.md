>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 8      |
|-----------------|
| Danny Duong 30120124                  |   
| Kevin Johnson 30124217                 |   
| Kaito Sugimura 30093204                |   
| Joshua Walters 30119430                |   


**Table of Contents**

[Introduction](#introduction)

[High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[How the pair testing was managed and team work/effort was
divided	](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[Difficulties encountered, challenges overcome, and lessons
learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[Comments/feedback on the lab and lab document itself](#commentsfeedback-on-the-lab-and-lab-document-itself)

# Introduction

This lab is conducted to acquire a comprehensive understanding of software testing. In it, we:

- familiarise ourselves with the system under test (SUT) and a defect tracking system.
- practice exploratory (manual non-scripted) testing.
- practice manual scripted testing.
- practice regression testing (re-testing a system after it has been changed).

We go into this lab with some background knowledge on exploratory and manual functional testing.

We know that exploratory testing involves designing and executing tests at the same time, and is reliant on domain knowledge of the program. There is no strict plan, but the human tester would try to create “realistic” scenarios that will cause the software to either fail or achieve the intended outcome. This involves varying inputs and state.

On the other hand, we know that manual functional testing has the tester following a script written by themselves or someone else. This script would outline test cases and test steps. There is no deviation from the path laid out in this script.

# High-level description of the exploratory testing plan

In systems like a banking system common bugs that can be found are:  
- Unexpected transitions to different pages
- Failure to detect invalid input
- Failure to perform expected transactions
- Failure to update information
- Failure display expected results
- Failure to record expected results
- Failure to respond in button presses

In order to address the issues that we have outlined above, as well as explore any other behaviours, we will set out to test and validate:
- All transitions between actions and pages
- Any inputs including expected user inputs and unexpected patterns of inputs
- Deposits and withdrawals of money and the accounts total balance
- Any text relating to the UI and the receipt 
- Button presses and their corresponding actions

We will simulate multiple expected customer behaviours as well as look into any possible bugs that could occur from unusual inputs. 
We will log the steps and conditions as we test the program to ensure the bugs are clear and repeatable to test in the future. 
# Comparison of exploratory and manual functional testing
Exploratory and manual functional testing have their advantages and disadvantages.

Exploratory testing is effective for discovering new or unusual bugs. There is little investment spent on preparation and reading documentation. However, coverage is difficult to track due to human bias and error. Additionally, exploratory testing does not scale up to complex systems and is not repeatable.

Manual functional testing can comprehensively confirm that a system functions as expected. It is better when the system’s requirements are well understood, and has specific functionality that needs to be tested. It allows for repeatable testing and enables regression testing, where the same tests can be run multiple times to ensure that new versions of the software do not break previously passing tests. However, there is high overhead in preparing test scripts and producing good documentation prior to test execution.

# Notes and discussion of the peer reviews of defect reports

After the two groups finished our pair exploratory testing we reviewed our two bug reports. We then compared reports and combined the lists. Many of the bugs we found were similar. However, below are the bugs we found that were unique to one of the reports..

Danny and Josh:
- Invalid Input in Balancing Inquiry Dispenses $20.
- After depositing money results in 10$ of the amount deposited.

Kaito and Kevin:
- Typo in Transaction Cancelled Message
- Incorrect transfer value displayed.
- Can deposit $0

# How the pair testing was managed and team work/effort was divided 

Our group split into 2 pairs to do the exploratory testing and each pair wrote their own separate brief bug reports. Once that was complete, our group came back together to share and review all discovered bugs. We proceeded to Manual Scripted Testing with one student as the main “driver” and other following along to report and input descriptive details about the bug. The rest of the work including Regression Testing and the reports were completed as a group. 

# Difficulties encountered, challenges overcome, and lessons learned

Communication was the most challenging part. Finding bugs individually was not very difficult, but coordinating our bug reports was. Writing descriptive details of the bugs, enough to let others recreate them, took a substantial amount of time. We tried solving this problem by creating a simple outline in describing the bugs (Function Tested, Initial State of System, How to Reproduce, Expected Outcome, and Actual Outcome). This allowed our team to keep our reports consistent and easy to read and follow. We have learned that planning a guideline before testing is very important to reduce confusion and time wasted in later stages.

# Comments/feedback on the lab and lab document itself

We enjoyed how the lab simulates a real testing environment. In particular, using the Jira software made bug reporting very quick and easy. However, we thought that the lab document was a bit unclear on certain things. Many of the sections seemed mismatched; for example, the submission guidelines coming before the introduction. The deliverables and grading seem to contradict the submission guidelines for what to submit. Some of the steps for Regression testing were not very clear as well. What to do on the Jira software was not very clear, and we had to work with assumptions a lot of the time. 

